{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Summary\n",
    "- bigscience/bloom-7b1\n",
    "- lora fine-tune bloom: 可插拔式的（plugin/adapter）\n",
    "    - freeeze original weights\n",
    "    - plugin lora adapters (peft)\n",
    "- huggingface transformers 库\n",
    "    - trainer.train 的参数及过程；\n",
    "    - mlm 与 clm 的差异：（都是 unsupervised learning，都可以自动地构建 input/labels）\n",
    "        - mlm：bert\n",
    "        - clm：gpt（bloom）\n",
    "    - pipeline\n",
    "        - dataset/tasks\n",
    "        - tokenizer\n",
    "        - training (fine-tune base lora)\n",
    "        - inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### base model & lora adapters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:16:43.020887Z",
     "start_time": "2023-07-10T06:16:42.977979Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:16:44.217368Z",
     "start_time": "2023-07-10T06:16:44.196074Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch       : 2.0.0+cu118\n",
      "bitsandbytes: 0.38.1\n",
      "sys         : 3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:16:44.725647Z",
     "start_time": "2023-07-10T06:16:44.704672Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/server/zhuyue/pretrained_model/bloom-7b1'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_dir = '/home/server/zhuyue/pretrained_model'\n",
    "model_path = os.path.join(pretrained_model_dir, 'bloom-7b1')\n",
    "model_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:16:47.665152Z",
     "start_time": "2023-07-10T06:16:47.621422Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "389f560ac1c44aa091ba2236e0c765d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, load_in_8bit=True, device_map='auto')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:17:03.719747Z",
     "start_time": "2023-07-10T06:16:48.881210Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:17:27.892180Z",
     "start_time": "2023-07-10T06:17:27.298079Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:17:28.460268Z",
     "start_time": "2023-07-10T06:17:28.453267Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "BloomForCausalLM(\n  (transformer): BloomModel(\n    (word_embeddings): Embedding(250880, 4096)\n    (word_embeddings_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n    (h): ModuleList(\n      (0-29): 30 x BloomBlock(\n        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n        (self_attention): BloomAttention(\n          (query_key_value): Linear8bitLt(in_features=4096, out_features=12288, bias=True)\n          (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n        (mlp): BloomMLP(\n          (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n          (gelu_impl): BloomGelu()\n          (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n        )\n      )\n    )\n    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=4096, out_features=250880, bias=False)\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:17:31.104214Z",
     "start_time": "2023-07-10T06:17:31.088460Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Embedding(250880, 4096)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:17:43.281008Z",
     "start_time": "2023-07-10T06:17:43.258527Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### freeze original weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float16"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0].dtype"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:19:16.937486Z",
     "start_time": "2023-07-10T06:19:16.894313Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "for i, params in enumerate(model.parameters()):\n",
    "    # freeze the model - train adapters later\n",
    "    params.requires_grad = False\n",
    "    if params.ndim == 1:\n",
    "        # cast the small parameters(e.g. layernorm) to fp32 for stability\n",
    "        params.data = params.data.to(torch.float32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:20:59.298871Z",
     "start_time": "2023-07-10T06:20:59.139798Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:21:02.321883Z",
     "start_time": "2023-07-10T06:21:02.315550Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:21:12.659243Z",
     "start_time": "2023-07-10T06:21:12.654454Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### lora adapters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:22:24.037322Z",
     "start_time": "2023-07-10T06:22:23.992514Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,  #low rank\n",
    "    lora_alpha=32,  #alpha scaling， scale lora weights/outputs\n",
    "    # target_modules=[\"q_proj\", \"v_proj\"], #if you know the\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"  # set this for CLM or Seq2Seq\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:22:40.084823Z",
     "start_time": "2023-07-10T06:22:40.080497Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7864320 || all params: 7076880384 || trainable%: 0.11112693126452029\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:22:54.399261Z",
     "start_time": "2023-07-10T06:22:44.162180Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): BloomForCausalLM(\n      (transformer): BloomModel(\n        (word_embeddings): Embedding(250880, 4096)\n        (word_embeddings_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n        (h): ModuleList(\n          (0-29): 30 x BloomBlock(\n            (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (self_attention): BloomAttention(\n              (query_key_value): Linear8bitLt(\n                in_features=4096, out_features=12288, bias=True\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=12288, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n            )\n            (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (mlp): BloomMLP(\n              (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n              (gelu_impl): BloomGelu()\n              (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n            )\n          )\n        )\n        (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n      )\n      (lm_head): CastOutputToFloat(\n        (0): Linear(in_features=4096, out_features=250880, bias=False)\n      )\n    )\n  )\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:23:00.776237Z",
     "start_time": "2023-07-10T06:23:00.738034Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### pipeline\n",
    "##### data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "dataset_dir = '/home/server/zhuyue/datasets'\n",
    "\n",
    "dataset_path = os.path.join(dataset_dir, 'english_quotes')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:23:32.433456Z",
     "start_time": "2023-07-10T06:23:32.392047Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/english_quotes to /home/server/.cache/huggingface/datasets/json/english_quotes-635ffa01a2541253/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef7538b4e13e4a97b41e5099a91ba109"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b86e6d73e344a71bae1f175ede2ead9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8af5c5f123e0417c8d864b276b26b173"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/server/.cache/huggingface/datasets/json/english_quotes-635ffa01a2541253/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6497874b062427982b2b9740af59be9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(dataset_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:39:34.749021Z",
     "start_time": "2023-07-10T06:39:33.755644Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['quote', 'author', 'tags'],\n        num_rows: 2508\n    })\n})"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:39:43.987667Z",
     "start_time": "2023-07-10T06:39:43.970796Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['quote', 'author', 'tags'],\n    num_rows: 2508\n})"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:39:44.541073Z",
     "start_time": "2023-07-10T06:39:44.531102Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  quote   \n0        “Be yourself; everyone else is already taken.”  \\\n1     “I'm selfish, impatient and a little insecure....   \n2     “Two things are infinite: the universe and hum...   \n3                      “So many books, so little time.”   \n4     “A room without books is like a body without a...   \n...                                                 ...   \n2503  “Morality is simply the attitude we adopt towa...   \n2504  “Don't aim at success. The more you aim at it ...   \n2505  “In life, finding a voice is speaking and livi...   \n2506  “Winter is the time for comfort, for good food...   \n2507                      “Silence is so freaking loud”   \n\n                     author                                               tags  \n0               Oscar Wilde  [be-yourself, gilbert-perreira, honesty, inspi...  \n1            Marilyn Monroe  [best, life, love, mistakes, out-of-control, t...  \n2           Albert Einstein  [human-nature, humor, infinity, philosophy, sc...  \n3               Frank Zappa                                     [books, humor]  \n4     Marcus Tullius Cicero                              [books, simile, soul]  \n...                     ...                                                ...  \n2503           Oscar Wilde,                             [morality, philosophy]  \n2504      Viktor E. Frankl,                               [happiness, success]  \n2505           John Grisham                               [inspirational-life]  \n2506          Edith Sitwell                            [comfort, home, winter]  \n2507          Sarah Dessen,    [just-listen, loud, owen, sara-dessen, silence]  \n\n[2508 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>quote</th>\n      <th>author</th>\n      <th>tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>“Be yourself; everyone else is already taken.”</td>\n      <td>Oscar Wilde</td>\n      <td>[be-yourself, gilbert-perreira, honesty, inspi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>“I'm selfish, impatient and a little insecure....</td>\n      <td>Marilyn Monroe</td>\n      <td>[best, life, love, mistakes, out-of-control, t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>“Two things are infinite: the universe and hum...</td>\n      <td>Albert Einstein</td>\n      <td>[human-nature, humor, infinity, philosophy, sc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>“So many books, so little time.”</td>\n      <td>Frank Zappa</td>\n      <td>[books, humor]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>“A room without books is like a body without a...</td>\n      <td>Marcus Tullius Cicero</td>\n      <td>[books, simile, soul]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2503</th>\n      <td>“Morality is simply the attitude we adopt towa...</td>\n      <td>Oscar Wilde,</td>\n      <td>[morality, philosophy]</td>\n    </tr>\n    <tr>\n      <th>2504</th>\n      <td>“Don't aim at success. The more you aim at it ...</td>\n      <td>Viktor E. Frankl,</td>\n      <td>[happiness, success]</td>\n    </tr>\n    <tr>\n      <th>2505</th>\n      <td>“In life, finding a voice is speaking and livi...</td>\n      <td>John Grisham</td>\n      <td>[inspirational-life]</td>\n    </tr>\n    <tr>\n      <th>2506</th>\n      <td>“Winter is the time for comfort, for good food...</td>\n      <td>Edith Sitwell</td>\n      <td>[comfort, home, winter]</td>\n    </tr>\n    <tr>\n      <th>2507</th>\n      <td>“Silence is so freaking loud”</td>\n      <td>Sarah Dessen,</td>\n      <td>[just-listen, loud, owen, sara-dessen, silence]</td>\n    </tr>\n  </tbody>\n</table>\n<p>2508 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].to_pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:39:45.041579Z",
     "start_time": "2023-07-10T06:39:45.004678Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "(['“Be yourself; everyone else is already taken.”',\n  \"“I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best.”\",\n  \"“Two things are infinite: the universe and human stupidity; and I'm not sure about the universe.”\",\n  '“So many books, so little time.”'],\n ['Oscar Wilde', 'Marilyn Monroe', 'Albert Einstein', 'Frank Zappa'])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['quote'][:4], dataset['train']['author'][:4]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:39:46.000025Z",
     "start_time": "2023-07-10T06:39:45.959176Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "{'quote': ['“Be yourself; everyone else is already taken.”',\n  \"“I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best.”\",\n  \"“Two things are infinite: the universe and human stupidity; and I'm not sure about the universe.”\",\n  '“So many books, so little time.”'],\n 'author': ['Oscar Wilde', 'Marilyn Monroe', 'Albert Einstein', 'Frank Zappa'],\n 'tags': [['be-yourself',\n   'gilbert-perreira',\n   'honesty',\n   'inspirational',\n   'misattributed-oscar-wilde',\n   'quote-investigator'],\n  ['best', 'life', 'love', 'mistakes', 'out-of-control', 'truth', 'worst'],\n  ['human-nature',\n   'humor',\n   'infinity',\n   'philosophy',\n   'science',\n   'stupidity',\n   'universe'],\n  ['books', 'humor']]}"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:4]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:39:47.057297Z",
     "start_time": "2023-07-10T06:39:47.047924Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2508 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79366bdd740a45b095cd03956cbe01ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def merge(row):\n",
    "    row['prediction'] = row['quote'] + ' ->: ' + str(row['tags'])\n",
    "    return row\n",
    "\n",
    "\n",
    "dataset['train'] = dataset['train'].map(merge)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:39:48.317239Z",
     "start_time": "2023-07-10T06:39:48.113734Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "[\"“Be yourself; everyone else is already taken.” ->: ['be-yourself', 'gilbert-perreira', 'honesty', 'inspirational', 'misattributed-oscar-wilde', 'quote-investigator']\",\n \"“I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best.” ->: ['best', 'life', 'love', 'mistakes', 'out-of-control', 'truth', 'worst']\",\n \"“Two things are infinite: the universe and human stupidity; and I'm not sure about the universe.” ->: ['human-nature', 'humor', 'infinity', 'philosophy', 'science', 'stupidity', 'universe']\",\n \"“So many books, so little time.” ->: ['books', 'humor']\"]"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['prediction'][:4]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:50.283694Z",
     "start_time": "2023-07-10T06:40:50.265275Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(dataset['train']['prediction'][:4])['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:51.391177Z",
     "start_time": "2023-07-10T06:40:51.377582Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2508 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b64de5b6b6f94c22a1d8fecbd6eb747c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda sample: tokenizer(sample['prediction']), batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:53.139547Z",
     "start_time": "2023-07-10T06:40:52.694155Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['quote', 'author', 'tags', 'prediction', 'input_ids', 'attention_mask'],\n        num_rows: 2508\n    })\n})"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:55.851465Z",
     "start_time": "2023-07-10T06:40:55.843650Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:40:56.804971Z",
     "start_time": "2023-07-10T06:40:56.799359Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/server/anaconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/200 : < :, Epoch 0.01/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=200, training_loss=2.3023052901029586, metrics={'train_runtime': 810.3407, 'train_samples_per_second': 3.949, 'train_steps_per_second': 0.247, 'total_flos': 1.3172999996964864e+16, 'train_loss': 2.3023052901029586, 'epoch': 1.28})"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        max_steps=200,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir='outputs'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T06:54:27.958358Z",
     "start_time": "2023-07-10T06:40:57.570009Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/server/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py:1405: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/home/server/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " “Training models with PEFT and Lora is coll” ->： ['EFT'] ->: ['EFT'] ->: ['EFT'] ->: ['EFT'] ->: ['EFT'] ->: ['EFT'] ->: ['EFT'] ->: ['EFT'] ->: ['EFT'] ->: ['EFT'] ->:\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(\"“Training models with PEFT and Lora is coll” ->：\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(**batch, max_new_tokens=50)\n",
    "\n",
    "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T07:05:26.340579Z",
     "start_time": "2023-07-10T07:05:18.467687Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " “An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains.” ->:  ['language-processing', 'nlp'] ->: ['natural-language-processing', 'nlp'] ->: ['natural-language-processing', 'nlp'] ->: ['natural-language-processing', 'nlp'] ->\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch = tokenizer(\n",
    "    \"“An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains.” ->: \",\n",
    "    return_tensors='pt')\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(**batch, max_new_tokens=50)\n",
    "\n",
    "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T07:05:40.366512Z",
     "start_time": "2023-07-10T07:05:33.001065Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "DataCollatorForLanguageModeling(tokenizer=BloomTokenizerFast(name_or_path='/home/server/zhuyue/pretrained_model/bloom-7b1', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False), mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.data_collator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T05:17:35.727535Z",
     "start_time": "2023-07-10T05:17:35.718232Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): BloomForCausalLM(\n      (transformer): BloomModel(\n        (word_embeddings): Embedding(250880, 4096)\n        (word_embeddings_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n        (h): ModuleList(\n          (0-29): 30 x BloomBlock(\n            (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (self_attention): BloomAttention(\n              (query_key_value): Linear8bitLt(\n                in_features=4096, out_features=12288, bias=True\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=12288, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n            )\n            (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (mlp): BloomMLP(\n              (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n              (gelu_impl): BloomGelu()\n              (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n            )\n          )\n        )\n        (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n      )\n      (lm_head): CastOutputToFloat(\n        (0): Linear(in_features=4096, out_features=250880, bias=False)\n      )\n    )\n  )\n)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T07:10:43.665272Z",
     "start_time": "2023-07-10T07:10:43.638821Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
